<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Abhishek Sah]]></title><description><![CDATA[Portfolio website hosting mostly blogs, thoughts & ideas]]></description><link>http://abhisheksah.xyz</link><generator>GatsbyJS</generator><lastBuildDate>Tue, 28 Apr 2020 06:44:32 GMT</lastBuildDate><item><title><![CDATA[What makes Kafka awesome?]]></title><description><![CDATA[Kafka is one of those software that are in the foundations of most of the data-driven companies e.g. LinkedIn, Uber, Spotify, Slack etc…]]></description><link>http://abhisheksah.xyz/what-makes-kafka-awesome/</link><guid isPermaLink="false">http://abhisheksah.xyz/what-makes-kafka-awesome/</guid><pubDate>Sat, 25 Apr 2020 22:12:03 GMT</pubDate><content:encoded>&lt;p&gt;&lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; is one of those software that are in the foundations of most of the data-driven companies e.g. LinkedIn, Uber, Spotify, Slack etc. Kafka solves the problem of high throughput real-time data pipelining with minimal latency. It’s flexible and lean architecture has helped companies scale massively without worrying about the quality and latency of the data pipeline. In this blog, we will go through internals of mighty Kafka and will find out what makes Kafka so awesome.&lt;/p&gt;
&lt;p&gt;Let’s begin with a subtle introduction.&lt;/p&gt;
&lt;h2&gt;What is Kafka&lt;/h2&gt;
&lt;p&gt;Kafka is mainly a Log processing application. By log, we mean any form of raw data or event that needs to be stored for later processing. For example, user activity events(page views, button clicks, logins, signups, likes, comments etc.) or operational metrics such as errors in applications, crashes, system usage etc. This processing can be either real-time or offline.&lt;/p&gt;
&lt;p&gt;Why do we need to store these raw data/events in the first place?&lt;/p&gt;
&lt;p&gt;Because this helps data-driven companies make efficient decisions. Companies can use this data to improve their search relevance, recommendation systems, and target appropriate ads to customers.&lt;/p&gt;
&lt;p&gt;This data, nowadays, can be of enormous volume. Think of the scale at which social media giants like Facebook, LinkedIn operate and storing all major events done by users on their website and mobile applications. This becomes billions of events every day.&lt;/p&gt;
&lt;p&gt;Storing these enormous data without compromising throughout is no simple job. Kafka is very good at this(we will see how). Moreover, Kafka gives you the ability to perform real-time processing on this data. One example of such real-time processing is calculating &lt;a href=&quot;https://www.davemanuel.com/investor-dictionary/surge-pricing/&quot;&gt;surge pricing&lt;/a&gt; in apps(like Uber).&lt;/p&gt;
&lt;p&gt;Surge pricing is calculated by taking into consideration the current demand and current supply. These two metrics are used in real-time to calculate how much surge price to add to the trip. For example, Surge price in Uber during rains in the evening.
The real-time usage of log data brings several challenges. One of which is high throughput. Kafka gives the ability to perform such real-time processing with minimal latency. Kafka is scalable, distributed, supports high throughput, supports real-time consumption of data and provides a &lt;a href=&quot;https://kafka.apache.org/documentation/#api&quot;&gt;rich set of APIs&lt;/a&gt; to make developers life easy.&lt;/p&gt;
&lt;h2&gt;Kafka Architecture&lt;/h2&gt;
&lt;p&gt;Kafka’s architecture is fairly simple. There are a few logical concepts that you need to understand in order to deep dive into Kafka. Kafka runs on &lt;a href=&quot;https://aws.amazon.com/pub-sub-messaging/&quot;&gt;Pub/Sub Model&lt;/a&gt;. Which means, there are entities which produce messages and put in some designated places and there are entities which subscribe to these places.&lt;/p&gt;
&lt;p&gt;Topic: Stream of messages of a particular type is called a topic. For example, a BookingLog Topic for Uber will keep all messages of trip booking type made on the Uber app.&lt;/p&gt;
&lt;p&gt;Producer: The entity which produces messages to a topic. The producer application constructs the messages and sends it to Kafka to store that message in a particular topic.&lt;/p&gt;
&lt;p&gt;Broker: Brokers are servers where the published messages are stored.
Consumer: The entity which consumes data that was put in a topic(from the Brokers). Consumers consume messages inside a topic from Broker via a bridge called Message Streams.&lt;/p&gt;
&lt;p&gt;Given these basic terminologies, you would have guessed the basic flow of data in Kafka.&lt;/p&gt;
&lt;p&gt;Producers produce messages(data) in topics. Brokers store those data. Consumers can consume data from these topics.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;&quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/86628d1bd5c7b4c66bf5da5effb8d0b0/dd507/1.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 18.91891891891892%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAIAAAABPYjBAAAACXBIWXMAAAsSAAALEgHS3X78AAAAuklEQVQI13WOzQqCQBSFfYgWrUIpgra9TpteLSinH6x0bfgKLYJsoY6CoUj+zIy/WNMoGW26fBzu4Z4Dl6PtvFo9P4qdgY8WOfxBMomX1U2+LXBRFCUIM6uH2WRrD1ZQECEPIC92gIYhgL2FOdc8Sp8kzTAheZ5zCCG20brSAzyVHAHAEYuuO8BHxxu7v7RmqluUZRzHYRgmScL9vm2gSnWzk4Nl3VOuvqz7ys3fX+7MMjQ3Zdcgr7/5N32Kwi2XMPMPAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Producer-Consumer&quot;
        title=&quot;Producer-Consumer&quot;
        src=&quot;/static/86628d1bd5c7b4c66bf5da5effb8d0b0/fcda8/1.png&quot;
        srcset=&quot;/static/86628d1bd5c7b4c66bf5da5effb8d0b0/12f09/1.png 148w,
/static/86628d1bd5c7b4c66bf5da5effb8d0b0/e4a3f/1.png 295w,
/static/86628d1bd5c7b4c66bf5da5effb8d0b0/fcda8/1.png 590w,
/static/86628d1bd5c7b4c66bf5da5effb8d0b0/efc66/1.png 885w,
/static/86628d1bd5c7b4c66bf5da5effb8d0b0/c83ae/1.png 1180w,
/static/86628d1bd5c7b4c66bf5da5effb8d0b0/dd507/1.png 1528w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Topics can be of very high throughput depending on the use cases. BookingsLog alone is a very high throughput topic if you think about Uber’s scale. So to balance the load, Topics are further divided into Partitions.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 586px;&quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/2769fcc22eb4e863ad7225e9dfe04ed5/a76f4/2.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 90.54054054054053%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAAsSAAALEgHS3X78AAACHUlEQVQ4y52US08TURTH5xOY6EJjsUVi8FUlMepC4+s76MpvYCSNG12pJLpQWfgJXIB2ppQKppi4ML4SQIhQUSriRDpPKsO8q7TTed3rGfoaUUPD5Lc48z/33HvPmX+GwFt6fIwRxgREOaV6e1a71R43Z7S+WY02HSgkZMs7lRV3PWGiJLtnM2IUu32gcH1ahUqEMPHVsLso7mCajw/zhzfjSIaPkewg/ROKPYSCa9/9qB8bEU4+E09sYFQICCmwpmdEuPRqpVj2cK1naF21PKnira4jlV2ligqy+S6Xn6EZTvslW/5qIytXvOU1F9bXi20ffS85i4bzrQ1ow8lrNuxSL4YB7E9zbfa8L8VdfLki1Yq/6HYnxcUz/NFMkAsD24EeDyk9T/kuiqWWGgPTq/6F58s7HzORJLs7RITkokNiNCVA0BQ7SHbbQKF3UobvFHwq2GPBsPs/6fdD9M+bfVPFxIuFG2+W7nyQHnw2mql7wJxeN8l/HYhcvKZiB27o/sOeqGHPt8XK1Ql5A72TSmJaT0xpELTECfnKuHztvUKbdnDyj7J7fFSMJJlOKnBfGDAsECNbyt4Uu2OwAN6uHU7A7buH+O40Dw498Cd/K4eG+Y4k+2ix1LLnw3njzJgIMz8f4mxWAM6NiUBThDWns+Ll1xKYrDUw0/YVy1PXUSqubiNeLY3P5XM0AwG8gljLapYHncpNe3poa38E/BuB2D/72m7hNQAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Partition&quot;
        title=&quot;Partition&quot;
        src=&quot;/static/2769fcc22eb4e863ad7225e9dfe04ed5/a76f4/2.png&quot;
        srcset=&quot;/static/2769fcc22eb4e863ad7225e9dfe04ed5/12f09/2.png 148w,
/static/2769fcc22eb4e863ad7225e9dfe04ed5/e4a3f/2.png 295w,
/static/2769fcc22eb4e863ad7225e9dfe04ed5/a76f4/2.png 586w&quot;
        sizes=&quot;(max-width: 586px) 100vw, 586px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Each broker stores one or more partitions of various topics.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;&quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/a4928c83ae4cfed322b03f1484bae121/29007/3.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 41.21621621621622%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAAsSAAALEgHS3X78AAABV0lEQVQY03VRz0sCQRjdv6C7RaduQp07BknHDlLQqUOXOgSeu3fv0LmCNDKlHxCu0A8UDKFTEkUQrqtW47qrTjntujszzvbNeinMx2Pgm29mvvfeKIwxypg3RNikAfgIwEUF2v4IUE9CCF8MQXYpVVxKb1EvpZEz/Rt4GqxQ5lDP9eT4f9/lwoemomEnnK6FEvpEohI61Mbj2mSiMnagxYoWnGnb3tZ9ay1vrBfMATcK5mrO2H36BN0KSM8jJ1kmRy/tvRLaL6H4s3VSJtm67VHaJO58BoVT9el0NZysAGdS+tRxNVZs9TlTPrpu9NqYu3yPZFBEbURUtKCi2fO37Qfs8yCzvm9zH9uugbsG/moR22aCMCFlN0lv885cuWksZ2uLF6/AJVWPXqGdR8yCxAKP4q9lWcq0+5wLIZMHD7jTsUwTxoEkh1IX8voFEGFZluM4g38C/AD12KTyT3EBAAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Partition&quot;
        title=&quot;Partition&quot;
        src=&quot;/static/a4928c83ae4cfed322b03f1484bae121/fcda8/3.png&quot;
        srcset=&quot;/static/a4928c83ae4cfed322b03f1484bae121/12f09/3.png 148w,
/static/a4928c83ae4cfed322b03f1484bae121/e4a3f/3.png 295w,
/static/a4928c83ae4cfed322b03f1484bae121/fcda8/3.png 590w,
/static/a4928c83ae4cfed322b03f1484bae121/efc66/3.png 885w,
/static/a4928c83ae4cfed322b03f1484bae121/c83ae/3.png 1180w,
/static/a4928c83ae4cfed322b03f1484bae121/29007/3.png 1600w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Topic BookingLog split into 3 partitions &amp;#x26; stored on 2 brokersPartitions of a topic are simply Log files. Think of all the partitions of a topic as a file on the storage system of the Brokers. Depending on the usage, this partition’s(Log file’s) size can vary. It can be very large for some high throughput topics. So it is &lt;a href=&quot;http://man7.org/linux/man-pages/man1/split.1.html&quot;&gt;split&lt;/a&gt; into multiple smaller files, called Segment Files.&lt;/p&gt;
&lt;p&gt;A partition now logically looks like:&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;&quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/a9db5c497734c6f25476158324c2b5e8/29007/4.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 35.13513513513513%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsSAAALEgHS3X78AAABfElEQVQY0x2QWU/CUBCF+6d98cHENTzof1AEAQlopQstlJ3S/XahhULZWlDBuGEUkkpwMLkP95tzZubci/X682hcSNzKybS83W4t+/Ey1oonpTscASrq5ComXN+IZN4CbPBe9EaEClfuAmI0ayM9WK/DyAWTuUcgm+3Zx+fPcYSimHal1ut7i/lieXCcK1XcAteZzd794G1vP9PtPWO8MGi2BvGkXG/2y1UXGUGDH8QSkqyOofkfvXhKNszpA2GoyK83vVRaFcQRGLCWOATT0RnJFB1BGmloghPG4SnZ5L1yzVW0cRZHJxFK032m4IjyKJ1VI+cMXAxrisEeURqBQ1LG0KCiCQiQ37JnLNdRdR/q2Xt9MHwhaEvRJoB4zrCdRxiHwTwy39bNgKDNUtWFQ7OQ1oc48Css53CVjqbvnsBVutBf5/saGmdwtNv88bliiyZBKcWSvfxav75904wOWKo6q3X4NF+StJajlGrdDX83fvBKUCqovOCF4eYPgcJLsqfHMiwAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Partition&quot;
        title=&quot;Partition&quot;
        src=&quot;/static/a9db5c497734c6f25476158324c2b5e8/fcda8/4.png&quot;
        srcset=&quot;/static/a9db5c497734c6f25476158324c2b5e8/12f09/4.png 148w,
/static/a9db5c497734c6f25476158324c2b5e8/e4a3f/4.png 295w,
/static/a9db5c497734c6f25476158324c2b5e8/fcda8/4.png 590w,
/static/a9db5c497734c6f25476158324c2b5e8/efc66/4.png 885w,
/static/a9db5c497734c6f25476158324c2b5e8/c83ae/4.png 1180w,
/static/a9db5c497734c6f25476158324c2b5e8/29007/4.png 1600w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Partition P2 split into 4 segment FilesLet’s say Partition P2 is the set of segment files S1, S2, S3 and S4. Any new message produced to Partition P2 will be written to the last segment file(here, say, S4).
This is to be noted that, at first the segment file lives in the memory of a broker. After a predefined threshold is reached, then only the segment file is flushed to the disk i.e. written on the secondary storage. There are two such predefined thresholds:
A certain number of messages have been published from the publisher of that partition.
Certain time has elapsed since last published event.&lt;/p&gt;
&lt;p&gt;The messages are only exposed to consumers when they are written to the disk. Messages residing in the memory of Brokers are never exposed.&lt;/p&gt;
&lt;h3&gt;Message format&lt;/h3&gt;
&lt;p&gt;Every message in Kafka is stored with an offset. This is so because the messages can be of varying size. So offset is decided by the size of the messages in the segment files.&lt;/p&gt;
&lt;p&gt;For example, Let’s say for segment file S1, three messages came from the producer which are of size 20bytes, 40 bytes and 20 bytes.
Then the ordering of message in segment file with offset typically looks like the image below with offset of M1 as 0, offset of M2 as 20 and offset of M3 as 40.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;&quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/9f139a53e0efd7d5291462bf5344a922/d777c/5.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 27.027027027027025%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAAsSAAALEgHS3X78AAAAj0lEQVQY012OUQ6EIBBDuf8BOYKJkPghgoCA+5ZGN9l+YKfTqTX3g+u6Sil6c85lotYqRSKoE5AxhnmPmfHFGHvv53m21ljjkwKXgZctWRh+x/iY932Hs8MtEkJ4DYhcEtcmDKkMRKIqWwd/tSEpJTlV8HvMx1q7rutxHKqKdVmW8UCd+TOlnHNq4b3ftu0DuWAkISWdWO8AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Message&quot;
        title=&quot;Message&quot;
        src=&quot;/static/9f139a53e0efd7d5291462bf5344a922/fcda8/5.png&quot;
        srcset=&quot;/static/9f139a53e0efd7d5291462bf5344a922/12f09/5.png 148w,
/static/9f139a53e0efd7d5291462bf5344a922/e4a3f/5.png 295w,
/static/9f139a53e0efd7d5291462bf5344a922/fcda8/5.png 590w,
/static/9f139a53e0efd7d5291462bf5344a922/efc66/5.png 885w,
/static/9f139a53e0efd7d5291462bf5344a922/c83ae/5.png 1180w,
/static/9f139a53e0efd7d5291462bf5344a922/d777c/5.png 1555w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A typical order of messages inside a segment file. Here you can see the offsets are in increasing order but they are not consecutive.&lt;/p&gt;
&lt;p&gt;Normal, message queue systems (such as RabbitMQ) use message ID instead of offset. The message ID does the one-to-one mapping of Message’s location on disk. This allows the user to do random indexing of the messages. Random access to messages is a &lt;a href=&quot;https://www.geeksforgeeks.org/difference-between-seek-time-and-rotational-latency-in-disk-scheduling/&quot;&gt;seek&lt;/a&gt; intensive operation. This is a costly IO operation affecting latency. Kafka avoids using message IDs.
Offsets are also used as consumers acknowledgements. If a consumer acknowledges an offset x, it is assumed that the consumer has consumed all messages with offset &amp;#x3C; x.&lt;/p&gt;
&lt;h3&gt;Managing partitions on the broker&lt;/h3&gt;
&lt;p&gt;It’s time to understand how brokers store and manage any partition. As we have understood, partitions are a set of segment files. Every segment file starts with some message. Every message is identified with a message offset.&lt;/p&gt;
&lt;p&gt;Brokers store a sorted table in memory which basically keeps the first offset of each segment file.&lt;/p&gt;
&lt;p&gt;Let’s see it this way. Let’s say for partition P1, there are 5 segment files(S1, S2, S3, S4 and S5). The first message in S1 has offset 0. Likewise, first messages of files S2, S3, S4 and S5 have offsets 1000, 2000, 2500 and 2700 respectively.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 306px;&quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/79d1bd16b9ce30ab7ade56a2422aabb0/98b92/6.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 138.51351351351352%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAcCAIAAADuuAg3AAAACXBIWXMAAAsSAAALEgHS3X78AAACL0lEQVQ4y42USY/aQBCF/f+FRACJU+CAOMEBInFCCCQOCMQqdgggCOuw70vY8uHOgI3tTN6hVe6qLr96XdVSKBRqt9v1ev2nHlqtlu5+o9FIp9NSs9m83++32+2uh+PxqN0UwfP5XOKfWJfL5abA9Xplc71eOxyO4XCIzc6bdzweS3AQPm3u0+kUDAaXy+UbNWFPJhOpUqmIuN8anM9n4ljf9gnmZzCSwuFwt9ul8pYaTRkII4w3Fxpns1kJt5Fgok4jwabTqY5gov5UKhWJRH7I8Pl8+/3+me4lmNHhfD7v9/tR+7uM7Xarc1hXbQGlclraD7Wj0Si6dTqdX2ogSecTfCoDsHu9XqFQkEqlEpkOh8NRjScXUYvSRTB0+v2+Dm3BqlqtohnpKT6XyymZv2jXajUsfFcF2HG5XHa73WazmUwms9lMqwpd8bL+FUz3nvmkNrT4kIGhpTabzaR4PM58jEajDzUWi8XsEwQoXQTDGcpSIpEgjq2xGhwQ5+cylC6CaS8aRIe2sEVGRoqThOrTNhLM7XajltPp/CZjtVq9CUZqQ8G4nmKxmEwmmZ5MJqO9qsefuczNZvOs7Qk6Ybfbse5lKF0EUw6DKcViMUoaDAZDNRhyelCsQBmADWdekX/NsxFetHUfQHZISmNSMAHlcplmfvb51yPp9XoDgYDH47FarRaLRcyzyPvqbaOnF6lFS9RlCLX/989f1vx4wyiMidFeFb5nb2Nrr4rX4g+3Axv3rYJ7XgAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Table&quot;
        title=&quot;Table&quot;
        src=&quot;/static/79d1bd16b9ce30ab7ade56a2422aabb0/98b92/6.png&quot;
        srcset=&quot;/static/79d1bd16b9ce30ab7ade56a2422aabb0/12f09/6.png 148w,
/static/79d1bd16b9ce30ab7ade56a2422aabb0/e4a3f/6.png 295w,
/static/79d1bd16b9ce30ab7ade56a2422aabb0/98b92/6.png 306w&quot;
        sizes=&quot;(max-width: 306px) 100vw, 306px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The broker will store an in-memory table which keeps the offset of the first message in each segment file. Here : 0, 1000, 2000, 2500, 2700&lt;/p&gt;
&lt;p&gt;Table of starting offset of each segment fileAny new message which comes, if stored in S5, won’t affect this table. As soon as, a new segment file appears, this table will be updated with the offset of the first message in the new segment file.
This table is used to find the segment file which contains the offset asked by the consumer.&lt;/p&gt;
&lt;h3&gt;Consumption of messages&lt;/h3&gt;
&lt;p&gt;Consuming messages is fairly simple. This is a 4 step process. 
Consumers send a pull request to brokers. In this request, there are two things. Consumers specify the offset to begin with, and an acceptable number of bytes in the messages.
Broker on receiving the offset to begin calculates which segment file has the message with asked offset. Simple binary search on the above table gives the address of the segment file to which the message belongs.
Broker fetches the data from that segment file and sends to the consumer.
The consumer consumes this data.&lt;/p&gt;
&lt;p&gt;The consumer sends the next offset in the new pull request.&lt;/p&gt;
&lt;h3&gt;Efficiency of Kafka&lt;/h3&gt;
&lt;p&gt;Kafka is considered the best real-time message log system. There are several design decisions(rather unconventional) which make Kafka so efficient.&lt;/p&gt;
&lt;h4&gt;Storage Efficiency&lt;/h4&gt;
&lt;p&gt;The first one is the use of offsets instead of message-ids which saves the cost of seek intensive random accesses. On top of that, keeping an index of the offsets of starting message in each segment file makes consumption from a partition fairly efficient.&lt;/p&gt;
&lt;p&gt;The second unconventional choice that we see in Kafka is avoiding any caching in Kafka Layer. Kafka avoids caching every message in memory at the process level. Instead, it relies on the underlying file system cache. This offers multiple benefits.&lt;/p&gt;
&lt;p&gt;First, it avoids double buffering. Modern operating heavily use main memory for disk caching. A modern OS will happily divert all free memory to disk caching with little performance penalty when the memory is reclaimed. All disk reads and writes will go through this unified cache. So even if a process maintains an in-process cache of the data, this data will likely be duplicated in OS page cache, effectively storing everything twice. Although there could be a debate on when and how to process level caching is useful. Read this &lt;a href=&quot;https://queue.acm.org/detail.cfm?id=1563874&quot;&gt;ACM article&lt;/a&gt; for a thorough analysis.&lt;/p&gt;
&lt;p&gt;Second, this cache(the file system cache) will stay warm even if the broker service is restarted, whereas the in-process cache will need to be rebuilt in memory.&lt;/p&gt;
&lt;p&gt;The third benefit is that no cache at process level means, less overhead in Garbage collection, giving efficiency in VM based languages.&lt;/p&gt;
&lt;p&gt;The fourth benefit comes from the use of traditional caching heuristics like write-through cache. Since both consumer and producer access the segment files sequentially, normal caching heuristics present in most OS works fine.&lt;/p&gt;
&lt;h4&gt;Network Efficiency&lt;/h4&gt;
&lt;p&gt;Consumption of messages from the File system involves fetching message from secondary storage and sending all the way to network sockets. This typically involves reading data from the storage to page cache, copying page cache to application buffer, copying application buffer to kernel buffer and finally sending kernel buffer to a socket. This involves 4 data copying calls and 2 system calls.&lt;/p&gt;
&lt;p&gt;Kafka uses the &lt;a href=&quot;http://man7.org/linux/man-pages/man2/sendfile.2.html&quot;&gt;sendfile API&lt;/a&gt; available in Linux which does the same thing in 2 data copying calls and 1 system call. This optimised API results in the fast consumption of data.&lt;/p&gt;
&lt;h4&gt;Broker’s design efficiency&lt;/h4&gt;
&lt;p&gt;Kafka Brokers are stateless. It means that a broker doesn’t know which consumer has consumed till what offset. The consumer itself keeps track of the offset. It reduces overhead from the Broker. But there are cons as well. Deleting messages becomes difficult since the broker doesn’t know if all consumers have consumed that message. For this, Kafka gives a time-based SLA where retention policy can be configured for brokers.&lt;/p&gt;
&lt;p&gt;Brokers being stateless gives a side benefit to consumers to deliberately choose to rewind back to an old offset and re-consume(if the messages have not exceeded retention period, of course). Though, it is a violation of queue but has proved as an essential feature. For example, if consumer application feeding on data from Kafka has some bugs, then the data can be replayed after the bug has been resolved.&lt;/p&gt;
&lt;h3&gt;Distributed Consensus&lt;/h3&gt;
&lt;p&gt;Let’s see how producers and consumers behave in a distributed setting.&lt;/p&gt;
&lt;p&gt;The producers produce data to a topic stored in Brokers and consumers consumer from these topics. Producers need to decide which partition to produce data. A partition can be randomly selected by the producer or by using a partitioning function.
Consumers to a topic are put in a group called “Consumer Group”. Each message m of Topic T is delivered to only one consumer in the consumer group. At any time, all messages from one single partition are consumed by a single consumer. Different consumer groups independently consume messages from topics. No coordination is required among them. But within a consumer group, coordination is required.&lt;/p&gt;
&lt;p&gt;The leader inside a consumer group is decided by a separate entity called &lt;a href=&quot;https://zookeeper.apache.org/&quot;&gt;Zookeeper&lt;/a&gt;. Kafka uses Zookeeper as a configuration store.
Zookeeper does these 3 things for Kafka.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detecting the addition and removal of brokers and consumers&lt;/li&gt;
&lt;li&gt;Triggering rebalance process in each consumer when the above event occurs&lt;/li&gt;
&lt;li&gt;Maintaining track of consumed offset of each partition.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Rebalancing, whenever required, is taken care of by zookeeper by running a rebalancing algorithm.
Kafka uses Zookeeper to maintain 4 types of registries(config stores)for various purposes. Let’s quickly see what each registry stores.&lt;/p&gt;
&lt;p&gt;Broker registry: Stores information about brokers. It stores the host and port of the broker and the set of topics and partitions stored in the broker.&lt;/p&gt;
&lt;p&gt;Consumer Registry: Stores consumer group of each consumer and set of topics each consumer is subscribing.&lt;/p&gt;
&lt;p&gt;Ownership Registry: Stores one path for every subscribed partition and the id of the consumer currently consuming for this
partition. This consumer is said to own the partition.&lt;/p&gt;
&lt;p&gt;Offset Registry: Stores for each partition, the offset of the last consumed message.&lt;/p&gt;
&lt;p&gt;The first three registries are ephemeral i,e. if the creating client is gone, these path created in corresponding registries is removed from by zookeeper server. Offset registry is persistent.&lt;/p&gt;
&lt;p&gt;Each consumer has a watcher on both Consumer registry and Broker registry. Whenever broker set changes or consumer group changes or the startup of a consumer happens the watcher notifies consumers to initiate a rebalancing algorithm process. This is to determine new subsets of partitions it should subscribe from. To learn more about the rebalancing algorithm, read this manual.&lt;/p&gt;
&lt;h3&gt;Delivery Guarantee&lt;/h3&gt;
&lt;p&gt;Kafka only guarantees at least once delivery. By design, they didn’t want to go for the exactly-once guarantee, because that would have retired efforts in two-phase commits which were not necessary requirement for Kafka.&lt;/p&gt;
&lt;p&gt;Kafka also doesn’t guarantee to order of the messages. Since messages go to different partitions, can be consumed from different partitions as per the consumers want. So there is no inherent ordering of messages by design in Kafka. Consumers itself will have to take care of the ordering of the messages.&lt;/p&gt;
&lt;p&gt;Kafka also takes care of the data corruption by giving Cyclic redundancy check at the message level. This allows you to check network error.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This blog was inspired by the &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2017/09/Kafka.pdf&quot;&gt;Kafka white paper&lt;/a&gt;. I would suggest the readers explore &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Kafka+papers+and+presentations&quot;&gt;this wiki&lt;/a&gt; to learn more detailed concepts about Kafka. Thanks for reading. Feel free to connect with me on &lt;a href=&quot;https://twitter.com/whoAbhishekSah&quot;&gt;Twitter&lt;/a&gt; for any conversations on this blog.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Books I read in 2019]]></title><description><![CDATA[I wanted to talk about the books I have read in 2019. I feel like I have learnt some good things from books and I feel like I should share…]]></description><link>http://abhisheksah.xyz/books-i-read-in-2019/</link><guid isPermaLink="false">http://abhisheksah.xyz/books-i-read-in-2019/</guid><pubDate>Sat, 14 Dec 2019 22:12:03 GMT</pubDate><content:encoded>&lt;p&gt;I wanted to talk about the books I have read in 2019. I feel like I have learnt some good things from books and I feel like I should share with everyone. So here we go!&lt;/p&gt;
&lt;p&gt;It has been a good year since I started realizing the power of reading books. I spend most of my times with books related to the things I like viz: software development, astronomy, personal finances among many other things. A detailed list of books that I read in 2019 is here on Goodreads. This blog talks about the books I loved the most and recommend everyone to read once if interested in that genre.&lt;/p&gt;
&lt;p&gt;I will mostly talk about the books of Personal Finances and Astronomy. Let’s start with Personal Finances first.&lt;/p&gt;
&lt;h3&gt;Personal Finances&lt;/h3&gt;
&lt;p&gt;Personal finances got my interest at the beginning of my senior year of college. I had taken a course called Financial Engineering and later on, I started liking the various concepts of finances(like options, trading, hedge fund market etc.). Later on, I read some books on personal finances. My personal favourite, on this topic, has been Rich Dad Poor Dad.&lt;/p&gt;
&lt;p&gt;This book changed my vision of how I think about finances and expenses. This book teaches us the major difference between the Rich and Poor, which is the mindset (and money follows that). This book teaches you the mindset required to be rich. The author presents very good real-world examples which support his arguments. A simple example, taken from the book:&lt;/p&gt;
&lt;p&gt;The poor and the middle class work for money. The rich have money work for them.
The book is filled with much such great advice which will change the way you look at your finances. Personally I agreed with almost all of the advice that was given in the book to be rich. But it is not an easy path to create wealth. The way we have been living our lives has conditioned us to the exact opposite(at least for me). So it requires a lot of discipline, and frequent self-introspection to ensure I am applying those learnings from this book.&lt;/p&gt;
&lt;p&gt;I would highly recommend this book for anyone who has never cared about personal finances(literally, complete beginners) and who are struggling to get their financial situations right.&lt;/p&gt;
&lt;p&gt;I was very keen to implement those learnings from Rich Dad Poor Dad, and so I started looking for books which teach what all financial instruments are available in India and their analysis. So the next book, I read was What Every Indian Should Know Before Investing. This is a great book for complete beginners who are not very familiar with the various financial instruments like Fixed Deposits(FD), Recurring Deposits(RD), Mutual Funds, Equities, and their tax implications. After gathering sufficient information, I started investing in multiple instruments. These books gave me enough exposure and confidence to take good care of my finances. Highly recommended!&lt;/p&gt;
&lt;h3&gt;Astronomy&lt;/h3&gt;
&lt;p&gt;Astronomy intrigues me on a very different level. I like to read about the discoveries and happening of the outer worlds. This subject is just so delightful to read about on any given day. Cosmos by Carl Sagan is such a treat for astronomy lovers. In a 384 pages long journey with Carl, you will discover many worlds like Venus, Mars, Jupiter, Saturn. You will find out what these worlds are made of, the possibility of intelligent lives on these worlds and their probable fate. I love the way he presents information about these world by logical inferences which are so simple to understand, they don’t require a very sound understanding of the Physical Sciences. After reading the books by Carl, I would say he is the best teacher I have got in astronomy. His books always spark curiosity in me to explore the unknown.&lt;/p&gt;
&lt;p&gt;Apart from these two subjects, I read a few books targeted on the contemporary political scenarios of India. The free Voice by Ravish Kumar is a good example which presents various incidents happened around India which reflect how India as a nation is changing democratically and culturally.&lt;/p&gt;
&lt;p&gt;So, overall it has been a good year. I learnt a lot of new things and applied them in real life. Looking forward to an even great year ahead!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Thanks for reading. Feel free to connect with me on Twitter for any conversations on this.&lt;/p&gt;</content:encoded></item></channel></rss>